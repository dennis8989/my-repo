{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google search original code without title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shorten_url\n",
      "  Downloading shorten_url-1.0.0-py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\devil\\anaconda3\\lib\\site-packages (from shorten_url) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\devil\\anaconda3\\lib\\site-packages (from requests>=2.23.0->shorten_url) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\devil\\anaconda3\\lib\\site-packages (from requests>=2.23.0->shorten_url) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\devil\\anaconda3\\lib\\site-packages (from requests>=2.23.0->shorten_url) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\devil\\anaconda3\\lib\\site-packages (from requests>=2.23.0->shorten_url) (2.10)\n",
      "Installing collected packages: shorten-url\n",
      "Successfully installed shorten-url-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install google\n",
    "!pip install shorten_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from googlesearch import search\n",
    "for url in search('\"nadal', stop=5,lang='zh-TW'):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Search with title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import ssl\n",
    "import re\n",
    "import shorten_url\n",
    "\n",
    "if sys.version_info[0] > 2:\n",
    "    from http.cookiejar import LWPCookieJar\n",
    "    from urllib.request import Request, urlopen\n",
    "    from urllib.parse import quote_plus, urlparse, parse_qs\n",
    "else:\n",
    "    from cookielib import LWPCookieJar\n",
    "    from urllib import quote_plus\n",
    "    from urllib2 import Request, urlopen\n",
    "    from urlparse import urlparse, parse_qs\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    is_bs4 = True\n",
    "except ImportError:\n",
    "    from BeautifulSoup import BeautifulSoup\n",
    "    is_bs4 = False\n",
    "\n",
    "__all__ = [\n",
    "\n",
    "    # Main search function.\n",
    "    'search',\n",
    "\n",
    "    # Shortcut for \"get lucky\" search.\n",
    "    'lucky',\n",
    "\n",
    "    # Miscellaneous utility functions.\n",
    "    'get_random_user_agent', 'get_tbs',\n",
    "]\n",
    "\n",
    "# URL templates to make Google searches.\n",
    "url_home = \"https://www.google.%(tld)s/\"\n",
    "url_search = \"https://www.google.%(tld)s/search?lr=lang_%(lang)s&\" \\\n",
    "             \"q=%(query)s&btnG=Google+Search&tbs=%(tbs)s&safe=%(safe)s&\" \\\n",
    "             \"cr=%(country)s&filter=0\"\n",
    "url_next_page = \"https://www.google.%(tld)s/search?lr=lang_%(lang)s&\" \\\n",
    "                \"q=%(query)s&start=%(start)d&tbs=%(tbs)s&safe=%(safe)s&\" \\\n",
    "                \"cr=%(country)s&filter=0\"\n",
    "url_search_num = \"https://www.google.%(tld)s/search?lr=lang_%(lang)s&\" \\\n",
    "                 \"q=%(query)s&num=%(num)d&btnG=Google+Search&tbs=%(tbs)s&\" \\\n",
    "                 \"&safe=%(safe)scr=%(country)s&filter=0\"\n",
    "url_next_page_num = \"https://www.google.%(tld)s/search?lr=lang_%(lang)s&\" \\\n",
    "                    \"q=%(query)s&num=%(num)d&start=%(start)d&tbs=%(tbs)s&\" \\\n",
    "                    \"safe=%(safe)s&cr=%(country)s&filter=0\"\n",
    "url_parameters = (\n",
    "    'hl', 'q', 'num', 'btnG', 'start', 'tbs', 'safe', 'cr', 'filter')\n",
    "\n",
    "# Cookie jar. Stored at the user's home folder.\n",
    "# If the cookie jar is inaccessible, the errors are ignored.\n",
    "home_folder = os.getenv('HOME')\n",
    "if not home_folder:\n",
    "    home_folder = os.getenv('USERHOME')\n",
    "    if not home_folder:\n",
    "        home_folder = '.'   # Use the current folder on error.\n",
    "cookie_jar = LWPCookieJar(os.path.join(home_folder, '.google-cookie'))\n",
    "try:\n",
    "    cookie_jar.load()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Default user agent, unless instructed by the user to change it.\n",
    "USER_AGENT = 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0)'\n",
    "\n",
    "# Load the list of valid user agents from the install folder.\n",
    "# The search order is:\n",
    "#   * user_agents.txt.gz\n",
    "#   * user_agents.txt\n",
    "#   * default user agent\n",
    "try:\n",
    "    install_folder = os.path.abspath(os.path.split(__file__)[0])\n",
    "    try:\n",
    "        user_agents_file = os.path.join(install_folder, 'user_agents.txt.gz')\n",
    "        import gzip\n",
    "        fp = gzip.open(user_agents_file, 'rb')\n",
    "        try:\n",
    "            user_agents_list = [_.strip() for _ in fp.readlines()]\n",
    "        finally:\n",
    "            fp.close()\n",
    "            del fp\n",
    "    except Exception:\n",
    "        user_agents_file = os.path.join(install_folder, 'user_agents.txt')\n",
    "        with open(user_agents_file) as fp:\n",
    "            user_agents_list = [_.strip() for _ in fp.readlines()]\n",
    "except Exception:\n",
    "    user_agents_list = [USER_AGENT]\n",
    "\n",
    "\n",
    "# Get a random user agent.\n",
    "def get_random_user_agent():\n",
    "    \"\"\"\n",
    "    Get a random user agent string.\n",
    "\n",
    "    :rtype: str\n",
    "    :return: Random user agent string.\n",
    "    \"\"\"\n",
    "    return random.choice(user_agents_list)\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to format the tbs parameter.\n",
    "def get_tbs(from_date, to_date):\n",
    "    \"\"\"\n",
    "    Helper function to format the tbs parameter.\n",
    "\n",
    "    :param datetime.date from_date: Python date object.\n",
    "    :param datetime.date to_date: Python date object.\n",
    "\n",
    "    :rtype: str\n",
    "    :return: Dates encoded in tbs format.\n",
    "    \"\"\"\n",
    "    from_date = from_date.strftime('%m/%d/%Y')\n",
    "    to_date = to_date.strftime('%m/%d/%Y')\n",
    "    return 'cdr:1,cd_min:%(from_date)s,cd_max:%(to_date)s' % vars()\n",
    "\n",
    "\n",
    "\n",
    "# Request the given URL and return the response page, using the cookie jar.\n",
    "# If the cookie jar is inaccessible, the errors are ignored.\n",
    "def get_page(url, user_agent=None, verify_ssl=True):\n",
    "    \"\"\"\n",
    "    Request the given URL and return the response page, using the cookie jar.\n",
    "\n",
    "    :param str url: URL to retrieve.\n",
    "    :param str user_agent: User agent for the HTTP requests.\n",
    "        Use None for the default.\n",
    "    :param bool verify_ssl: Verify the SSL certificate to prevent\n",
    "        traffic interception attacks. Defaults to True.\n",
    "\n",
    "    :rtype: str\n",
    "    :return: Web page retrieved for the given URL.\n",
    "\n",
    "    :raises IOError: An exception is raised on error.\n",
    "    :raises urllib2.URLError: An exception is raised on error.\n",
    "    :raises urllib2.HTTPError: An exception is raised on error.\n",
    "    \"\"\"\n",
    "    if user_agent is None:\n",
    "        user_agent = USER_AGENT\n",
    "    request = Request(url)\n",
    "    request.add_header('User-Agent', user_agent)\n",
    "    cookie_jar.add_cookie_header(request)\n",
    "    if verify_ssl:\n",
    "        response = urlopen(request)\n",
    "    else:\n",
    "        context = ssl._create_unverified_context()\n",
    "        response = urlopen(request, context=context)\n",
    "    cookie_jar.extract_cookies(response, request)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    try:\n",
    "        cookie_jar.save()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return html\n",
    "\n",
    "\n",
    "# Filter links found in the Google result pages HTML code.\n",
    "# Returns None if the link doesn't yield a valid result.\n",
    "def filter_result(link):\n",
    "    try:\n",
    "\n",
    "        # Decode hidden URLs.\n",
    "        if link.startswith('/url?'):\n",
    "            o = urlparse(link, 'http')\n",
    "            link = parse_qs(o.query)['q'][0]\n",
    "\n",
    "        # Valid results are absolute URLs not pointing to a Google domain,\n",
    "        # like images.google.com or googleusercontent.com for example.\n",
    "        # TODO this could be improved!\n",
    "        o = urlparse(link, 'http')\n",
    "        if o.netloc and 'google' not in o.netloc:\n",
    "            return link\n",
    "\n",
    "    # On error, return None.\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Returns a generator that yields URLs.\n",
    "def search_1(query, tld='com', lang='en', tbs='0', safe='off', num=10, start=0,\n",
    "           stop=None, pause=2.0, country='', extra_params=None,\n",
    "           user_agent=None, verify_ssl=True):\n",
    "    \"\"\"\n",
    "    Search the given query string using Google.\n",
    "\n",
    "    :param str query: Query string. Must NOT be url-encoded.\n",
    "    :param str tld: Top level domain.\n",
    "    :param str lang: Language.\n",
    "    :param str tbs: Time limits (i.e \"qdr:h\" => last hour,\n",
    "        \"qdr:d\" => last 24 hours, \"qdr:m\" => last month).\n",
    "    :param str safe: Safe search.\n",
    "    :param int num: Number of results per page.\n",
    "    :param int start: First result to retrieve.\n",
    "    :param int stop: Last result to retrieve.\n",
    "        Use None to keep searching forever.\n",
    "    :param float pause: Lapse to wait between HTTP requests.\n",
    "        A lapse too long will make the search slow, but a lapse too short may\n",
    "        cause Google to block your IP. Your mileage may vary!\n",
    "    :param str country: Country or region to focus the search on. Similar to\n",
    "        changing the TLD, but does not yield exactly the same results.\n",
    "        Only Google knows why...\n",
    "    :param dict extra_params: A dictionary of extra HTTP GET\n",
    "        parameters, which must be URL encoded. For example if you don't want\n",
    "        Google to filter similar results you can set the extra_params to\n",
    "        {'filter': '0'} which will append '&filter=0' to every query.\n",
    "    :param str user_agent: User agent for the HTTP requests.\n",
    "        Use None for the default.\n",
    "    :param bool verify_ssl: Verify the SSL certificate to prevent\n",
    "        traffic interception attacks. Defaults to True.\n",
    "\n",
    "    :rtype: generator of str\n",
    "    :return: Generator (iterator) that yields found URLs.\n",
    "        If the stop parameter is None the iterator will loop forever.\n",
    "    \"\"\"\n",
    "    # Set of hashes for the results found.\n",
    "    # This is used to avoid repeated results.\n",
    "    hashes = set()\n",
    "\n",
    "    # Count the number of links yielded.\n",
    "    count = 0\n",
    "\n",
    "    # Prepare the search string.\n",
    "    query = quote_plus(query)\n",
    "\n",
    "    # If no extra_params is given, create an empty dictionary.\n",
    "    # We should avoid using an empty dictionary as a default value\n",
    "    # in a function parameter in Python.\n",
    "    if not extra_params:\n",
    "        extra_params = {}\n",
    "\n",
    "    # Check extra_params for overlapping.\n",
    "    for builtin_param in url_parameters:\n",
    "        if builtin_param in extra_params.keys():\n",
    "            raise ValueError(\n",
    "                'GET parameter \"%s\" is overlapping with \\\n",
    "                the built-in GET parameter',\n",
    "                builtin_param\n",
    "            )\n",
    "\n",
    "    # Grab the cookie from the home page.\n",
    "    get_page(url_home % vars(), user_agent, verify_ssl)\n",
    "\n",
    "    # Prepare the URL of the first request.\n",
    "    if start:\n",
    "        if num == 10:\n",
    "            url = url_next_page % vars()\n",
    "        else:\n",
    "            url = url_next_page_num % vars()\n",
    "    else:\n",
    "        if num == 10:\n",
    "            url = url_search % vars()\n",
    "        else:\n",
    "            url = url_search_num % vars()\n",
    "\n",
    "    # Loop until we reach the maximum result, if any (otherwise, loop forever).\n",
    "    while not stop or count < stop:\n",
    "\n",
    "        # Remember last count to detect the end of results.\n",
    "        last_count = count\n",
    "\n",
    "        # Append extra GET parameters to the URL.\n",
    "        # This is done on every iteration because we're\n",
    "        # rebuilding the entire URL at the end of this loop.\n",
    "        for k, v in extra_params.items():\n",
    "            k = quote_plus(k)\n",
    "            v = quote_plus(v)\n",
    "            url = url + ('&%s=%s' % (k, v))\n",
    "\n",
    "        # Sleep between requests.\n",
    "        # Keeps Google from banning you for making too many requests.\n",
    "        time.sleep(pause)\n",
    "\n",
    "        # Request the Google Search results page.\n",
    "        html = get_page(url, user_agent, verify_ssl)\n",
    "\n",
    "        # Parse the response and get every anchored URL.\n",
    "        if is_bs4:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "        else:\n",
    "            soup = BeautifulSoup(html)\n",
    "        try:\n",
    "            anchors = soup.find(id='search').findAll('a')\n",
    "            # Sometimes (depending on the User-agent) there is\n",
    "            # no id \"search\" in html response...\n",
    "        except AttributeError:\n",
    "            # Remove links of the top bar.\n",
    "            gbar = soup.find(id='gbar')\n",
    "            if gbar:\n",
    "                gbar.clear()\n",
    "            anchors = soup.findAll('a')\n",
    "\n",
    "        # Process every anchored URL.\n",
    "        for a in anchors:\n",
    "\n",
    "            # Get the URL from the anchor tag.\n",
    "            try:\n",
    "                link = a['href']\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            # Filter invalid links and links pointing to Google itself.\n",
    "            link = filter_result(link)\n",
    "            if not link:\n",
    "                continue\n",
    "\n",
    "            # Discard repeated results.\n",
    "            h = hash(link)\n",
    "            if h in hashes:\n",
    "                continue\n",
    "            hashes.add(h)\n",
    "            title = a.text\n",
    "            # Yield the result.\n",
    "            yield link, title\n",
    "\n",
    "            # Increase the results counter.\n",
    "            # If we reached the limit, stop.\n",
    "            count += 1\n",
    "            if stop and count >= stop:\n",
    "                return\n",
    "\n",
    "        # End if there are no more results.\n",
    "        # XXX TODO review this logic, not sure if this is still true!\n",
    "        if last_count == count:\n",
    "            break\n",
    "\n",
    "        # Prepare the URL for the next request.\n",
    "        start += num\n",
    "        if num == 10:\n",
    "            url = url_next_page % vars()\n",
    "        else:\n",
    "            url = url_next_page_num % vars()\n",
    "\n",
    "\n",
    "\n",
    "# Shortcut to single-item search.\n",
    "# Evaluates the iterator to return the single URL as a string.\n",
    "def lucky(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Shortcut to single-item search.\n",
    "\n",
    "    Same arguments as the main search function, but the return value changes.\n",
    "\n",
    "    :rtype: str\n",
    "    :return: URL found by Google.\n",
    "    \"\"\"\n",
    "    return next(search(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "維基百科\n",
      "https://zh.wikipedia.org/zh-tw/%E6%8B%89%E6%96%90%E7%88%BE%C2%B7%E6%8B%BF%E5%BA%A6\n",
      "\n",
      "家庭和早年生活\n",
      "https://zh.wikipedia.org/zh-tw/%E6%8B%89%E6%96%90%E7%88%BE%C2%B7%E6%8B%BF%E5%BA%A6#%E5%AE%B6%E5%BA%AD%E5%92%8C%E6%97%A9%E5%B9%B4%E7%94%9F%E6%B4%BB\n",
      "\n",
      "职业生涯\n",
      "https://zh.wikipedia.org/zh-tw/%E6%8B%89%E6%96%90%E7%88%BE%C2%B7%E6%8B%BF%E5%BA%A6#%E8%81%8C%E4%B8%9A%E7%94%9F%E6%B6%AF\n",
      "\n",
      "技術分析\n",
      "https://zh.wikipedia.org/zh-tw/%E6%8B%89%E6%96%90%E7%88%BE%C2%B7%E6%8B%BF%E5%BA%A6#%E6%8A%80%E8%A1%93%E5%88%86%E6%9E%90\n",
      "\n",
      "更多資訊如下\n",
      "https://www.google.com/search?q=nadal\n",
      "0:00:07.599547\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import singledispatch_helpers\n",
    "starttime = datetime.datetime.now() #開始計算時間\n",
    "aa = search_1('nadal', stop=5, lang='ZH-TW')\n",
    "result = ''\n",
    "for i in aa:\n",
    "    if i[1] != '':\n",
    "        title = re.split(r\"›\", i[1])\n",
    "        title = title[0].rsplit(' ',2)\n",
    "        result = result + title[0] +'\\n' + i[0]+\"\\n\\n\"\n",
    "result = result + '更多資訊如下\\n'+'https://www.google.com/search?q='+'nadal'\n",
    "print(result)\n",
    "import time\n",
    "time.sleep(5)\n",
    "endtime = datetime.datetime.now() #結束時間\n",
    "print(endtime - starttime) #程式執行時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gsearch\n",
      "  Downloading gsearch-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Installing collected packages: gsearch\n",
      "Successfully installed gsearch-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsearch.googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = search('nadal', num_results=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results where found. Did the rate limit exceed?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
